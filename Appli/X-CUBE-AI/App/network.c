/* AUTOGENERATED DO NOT MODIFY */

/**
  ******************************************************************************
  * @file    network.c
  * @brief   NN Code autogenerated DO NOT MODIFY IT
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2023 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

/*
 * GIT_SHA         "e619e8606099384540d70eeaaa8091752b1bebe9"
 * GIT_BRANCH      "STAI-2.2"
 * GIT_DESCRIPTION "atonn-v1.1.1-14-ge619e8606"
 *
 * BUILD_DIR       "/c/local/jenkins_cloud/workspace/2-STEDGEAI_BuildAtonnExe_Win/git/onnx_backend/build"
 * BUILD_DATE      "25/06/2025"
 * BUILD_AUTHOR    "aitest"
 *
 * Command Line options:
 * --load-mdesc-file = "C:/ST/STEdgeAI/2.2/scripts/N6_scripts/my_mdescs/stm32n6"
 * --load-mpool-file = "C:/ST/STEdgeAI/2.2/scripts/N6_scripts/my_mpools/stm32n6"
 * --cache-maintenance = true
 * --enable-virtual-mem-pools = true
 * --native-float = true
 * --json-quant-file = "C:/Users/willi/.stm32cubemx/network_output/mnist_model_OE_3_3_0_Q.json"
 * --optimization = 3
 * --Os = true
 * --Omax-ca-pipe = 4
 * --Ocache-opt = true
 * --output-info-file = "c_info"
 * --onnx-input = "C:/Users/willi/.stm32cubemx/network_output/mnist_model_OE_3_3_0.onnx"
 * --out-dir-prefix = "C:/Users/willi/AppData/Local/Temp/mxAI_workspace24097826693310014344002952092208330/neural_art__network/"
 * --all-buffers-info = true
 * --mvei = true
 * --Oauto-sched = true
 *
 * auto* option expanded into:
 *   alt-scheduler = false
 */

#include "ll_aton_NN_interface.h"
#include "ll_aton.h"
#include "ll_aton_lib.h"
#include "ll_aton_version.h"
#include "ll_sw.h"

#if LL_ATON_VERSION_MAJOR != 1 || LL_ATON_VERSION_MINOR != 1 || LL_ATON_VERSION_MICRO != 1 || LL_ATON_VERSION_DEV != 14
#  warning "Possible mismatch in ll_aton library used"
#endif

#if !defined(LL_ATON_DBG_BUFFER_INFO_EXCLUDED)
#  define LL_ATON_DBG_BUFFER_INFO_EXCLUDED 0
#endif

/* global pool 7 is ? */
/* index=7 file postfix=xSPI1 name=hyperRAM offset=0x90000000  absolute_mode size=33554424 READ_WRITE THROUGHPUT=MID LATENCY=HIGH byte width=2 freq ratio=5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=380 write_power=340 use4initializers=YES score=82  */
/* global pool 8 is 241.05 KB */
/* index=8 file postfix=xSPI2 name=octoFlash offset=0x71000000  absolute_mode size=117440504 READ_ONLY THROUGHPUT=MID LATENCY=HIGH byte width=1 freq ratio=6 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=110 write_power=400 use4initializers=YES score=50  */
/* global pool 1 is 26.03 KB */
/* index=1 file postfix=AXISRAM5 name=npuRAM5 offset=0x342e0000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 2 is ? */
/* index=2 file postfix=AXISRAM4 name=npuRAM4 offset=0x34270000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 3 is ? */
/* index=3 file postfix=AXISRAM3 name=npuRAM3 offset=0x34200000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 0 is ? */
/* index=0 file postfix=AXISRAM6 name=npuRAM6 offset=0x34350000  absolute_mode size=458744 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=15.79 use4initializers=NO score=94  */
/* global pool 11 is 1.90 MB */
/* index=11 file postfix=AXISRAM2_AXISRAM3_AXISRAM4_AXISRAM5_AXISRAM6 name=cpuRAM2_npuRAM3_npuRAM4_npuRAM5_npuRAM6 offset=0x34100000  absolute_mode size=2883576 vpool READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=16.201 use4initializers=NO score=85  */
/* global pool 4 is ? */
/* index=4 file postfix=AXISRAM2 name=cpuRAM2 offset=0x34100000  absolute_mode size=1048576 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=17.324 write_power=15.321 use4initializers=NO score=84  */
/* global pool 5 is ? */
/* index=5 file postfix=AXISRAM1 name=cpuRAM1 offset=0x34064000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=16.616 write_power=14.522 use4initializers=NO score=84  */
/* global pool 6 is ? */
/* index=6 file postfix=AXIFLEXMEM name=flexMEM offset=0x34000000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=9.381 write_power=8.569 use4initializers=NO score=84  */

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Input_Buffer_Default(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Input_Buffer_Default(uint32_t num)
{
  { 
    return NULL;
  }
}

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Output_Buffer_Default(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Output_Buffer_Default(uint32_t num)
{
  { 
    return NULL;
  }
}

bool LL_ATON_EC_Network_Init_Default(void)
{
  return true;
}

bool LL_ATON_EC_Inference_Init_Default(void)
{
  return true;
}

/* scheduling epoch=0    nodes=11  ------------------------------------------------------------------- */

/* scheduling epoch=1    nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=2    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_2(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_2 */
  Conv_sw_info conv1_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 784,
    .general.input.stride.b = 3136,
    .general.input.stride.h = 112,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 6,
    .weights.dim.tensor_h = 5,
    .weights.dim.tensor_w = 5,
    .weights.dim.tensor_c = 1,
    .weights.dim.num_elem = 150,
    .weights.stride.b = 100,
    .weights.stride.h = 20,
    .weights.stride.w = 4,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 245280))) /* Equivalent hex address = 0x7103be20UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 6,
    .bias.dim.num_elem = 6,
    .bias.stride.b = 24,
    .bias.stride.h = 24,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 246816))) /* Equivalent hex address = 0x7103c420UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 6,
    .general.output.dim.num_elem = 4704,
    .general.output.stride.b = 18816,
    .general.output.stride.h = 672,
    .general.output.stride.w = 24,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 3136))) /* Equivalent hex address = 0x342e0c40UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {2, 2, 2, 2},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_2 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv1_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 3136))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 21952))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 3136))) /* Equivalent hex address = 0x342e0c40UL */, 18816);

}


/* scheduling epoch=3    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_3(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Tanh node=Tanh_3 */
  Activ_sw_info activ2_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 6,
    .general.input.dim.num_elem = 4704,
    .general.input.stride.b = 18816,
    .general.input.stride.h = 672,
    .general.input.stride.w = 24,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 3136))) /* Equivalent hex address = 0x342e0c40UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 6,
    .general.output.dim.num_elem = 4704,
    .general.output.stride.b = 18816,
    .general.output.stride.h = 672,
    .general.output.stride.w = 24,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 3136))) /* Equivalent hex address = 0x342e0c40UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_TANH,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Tanh_3 mapped on EmbedNets (FLOAT) as Tanh | Category: Computational */
  ll_sw_forward_activ(&activ2_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 3136))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 21952))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 3136))) /* Equivalent hex address = 0x342e0c40UL */, 18816);

}


/* scheduling epoch=4    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_4(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=AveragePool node=AveragePool_4 */
  Pool_sw_info pool3_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 6,
    .general.input.dim.num_elem = 4704,
    .general.input.stride.b = 18816,
    .general.input.stride.h = 672,
    .general.input.stride.w = 24,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 3136))) /* Equivalent hex address = 0x342e0c40UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 6,
    .general.output.dim.num_elem = 1176,
    .general.output.stride.b = 4704,
    .general.output.stride.h = 336,
    .general.output.stride.w = 24,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 21952))) /* Equivalent hex address = 0x342e55c0UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 0, 0},
    .strides = {2, 2},
    .k_shape = {2, 2},
    .count_include_pad = 0,
    .general.type = LL_SW_AVGPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node AveragePool_4 mapped on EmbedNets (FLOAT) as AveragePool | Category: Computational */
  ll_sw_forward_pool(&pool3_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 21952))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 26656))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 21952))) /* Equivalent hex address = 0x342e55c0UL */, 4704);

}


/* scheduling epoch=5    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_5(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_5 */
  Conv_sw_info conv4_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 6,
    .general.input.dim.num_elem = 1176,
    .general.input.stride.b = 4704,
    .general.input.stride.h = 336,
    .general.input.stride.w = 24,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 21952))) /* Equivalent hex address = 0x342e55c0UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 16,
    .weights.dim.tensor_h = 5,
    .weights.dim.tensor_w = 5,
    .weights.dim.tensor_c = 6,
    .weights.dim.num_elem = 2400,
    .weights.stride.b = 600,
    .weights.stride.h = 120,
    .weights.stride.w = 24,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 232320))) /* Equivalent hex address = 0x71038b80UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 16,
    .bias.dim.num_elem = 16,
    .bias.stride.b = 64,
    .bias.stride.h = 64,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 246704))) /* Equivalent hex address = 0x7103c3b0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 1600,
    .general.output.stride.b = 6400,
    .general.output.stride.h = 640,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_5 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv4_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 6400);

}


/* scheduling epoch=6    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_6(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Tanh node=Tanh_6 */
  Activ_sw_info activ5_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 1600,
    .general.input.stride.b = 6400,
    .general.input.stride.h = 640,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 1600,
    .general.output.stride.b = 6400,
    .general.output.stride.h = 640,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_TANH,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Tanh_6 mapped on EmbedNets (FLOAT) as Tanh | Category: Computational */
  ll_sw_forward_activ(&activ5_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 6400);

}


/* scheduling epoch=7    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_7(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=AveragePool node=AveragePool_7 */
  Pool_sw_info pool6_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 1600,
    .general.input.stride.b = 6400,
    .general.input.stride.h = 640,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 5,
    .general.output.dim.tensor_w = 5,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 400,
    .general.output.stride.b = 1600,
    .general.output.stride.h = 320,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6400))) /* Equivalent hex address = 0x342e1900UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 0, 0},
    .strides = {2, 2},
    .k_shape = {2, 2},
    .count_include_pad = 0,
    .general.type = LL_SW_AVGPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node AveragePool_7 mapped on EmbedNets (FLOAT) as AveragePool | Category: Computational */
  ll_sw_forward_pool(&pool6_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 8000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6400))) /* Equivalent hex address = 0x342e1900UL */, 1600);

}


/* scheduling epoch=8    nodes=2   ------------------------------------------------------------------- */

/* scheduling epoch=9    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_9(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Gemm_10_conv_4 */
  Conv_sw_info conv7_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 400,
    .general.input.dim.num_elem = 400,
    .general.input.stride.b = 1600,
    .general.input.stride.h = 1600,
    .general.input.stride.w = 1600,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6400))) /* Equivalent hex address = 0x342e1900UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 120,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 400,
    .weights.dim.num_elem = 48000,
    .weights.stride.b = 1600,
    .weights.stride.h = 1600,
    .weights.stride.w = 1600,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 0))) /* Equivalent hex address = 0x71000000UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 120,
    .general.output.dim.num_elem = 120,
    .general.output.stride.b = 480,
    .general.output.stride.h = 480,
    .general.output.stride.w = 480,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Gemm_10_conv_4 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv7_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 480);

}


/* scheduling epoch=10   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=11   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_11(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Gemm_10_add_6 */
  Arith_sw_info arith8_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 120,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 120,
    .general.input.stride.b = 480,
    .general.input.stride.h = 480,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 120,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 120,
    .operand.stride.b = 480,
    .operand.stride.h = 480,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 245888))) /* Equivalent hex address = 0x7103c080UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 120,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 120,
    .general.output.stride.b = 480,
    .general.output.stride.h = 480,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) /* Equivalent hex address = 0x342e01e0UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Gemm_10_add_6 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith8_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 960))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) /* Equivalent hex address = 0x342e01e0UL */, 480);

}


/* scheduling epoch=12   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_12(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Tanh node=Tanh_11 */
  Activ_sw_info activ9_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 120,
    .general.input.dim.num_elem = 120,
    .general.input.stride.b = 480,
    .general.input.stride.h = 480,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) /* Equivalent hex address = 0x342e01e0UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 120,
    .general.output.dim.num_elem = 120,
    .general.output.stride.b = 480,
    .general.output.stride.h = 480,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) /* Equivalent hex address = 0x342e01e0UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_TANH,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Tanh_11 mapped on EmbedNets (FLOAT) as Tanh | Category: Computational */
  ll_sw_forward_activ(&activ9_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 960))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) /* Equivalent hex address = 0x342e01e0UL */, 480);

}


/* scheduling epoch=13   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=14   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_14(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) /* Equivalent hex address = 0x342e0140UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Gemm_12_conv_11 */
  Conv_sw_info conv10_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 120,
    .general.input.dim.num_elem = 120,
    .general.input.stride.b = 480,
    .general.input.stride.h = 480,
    .general.input.stride.w = 480,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 480))) /* Equivalent hex address = 0x342e01e0UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 84,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 120,
    .weights.dim.num_elem = 10080,
    .weights.stride.b = 480,
    .weights.stride.h = 480,
    .weights.stride.w = 480,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 192000))) /* Equivalent hex address = 0x7102ee00UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 84,
    .general.output.dim.num_elem = 84,
    .general.output.stride.b = 336,
    .general.output.stride.h = 336,
    .general.output.stride.w = 336,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Gemm_12_conv_11 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv10_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 352);

}


/* scheduling epoch=15   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=16   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_16(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) /* Equivalent hex address = 0x342e0140UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Gemm_12_add_13 */
  Arith_sw_info arith11_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 84,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 84,
    .general.input.stride.b = 336,
    .general.input.stride.h = 336,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 84,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 84,
    .operand.stride.b = 336,
    .operand.stride.h = 336,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 246368))) /* Equivalent hex address = 0x7103c260UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 84,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 84,
    .general.output.stride.b = 336,
    .general.output.stride.h = 336,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 336))) /* Equivalent hex address = 0x342e0150UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Gemm_12_add_13 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith11_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 672))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) /* Equivalent hex address = 0x342e0140UL */, 352);

}


/* scheduling epoch=17   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_17(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) /* Equivalent hex address = 0x342e0140UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Tanh node=Tanh_13 */
  Activ_sw_info activ12_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 84,
    .general.input.dim.num_elem = 84,
    .general.input.stride.b = 336,
    .general.input.stride.h = 336,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 336))) /* Equivalent hex address = 0x342e0150UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 84,
    .general.output.dim.num_elem = 84,
    .general.output.stride.b = 336,
    .general.output.stride.h = 336,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 336))) /* Equivalent hex address = 0x342e0150UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_TANH,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Tanh_13 mapped on EmbedNets (FLOAT) as Tanh | Category: Computational */
  ll_sw_forward_activ(&activ12_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 672))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 320))) /* Equivalent hex address = 0x342e0140UL */, 352);

}


/* scheduling epoch=18   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=19   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_19(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start or end address (only line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 64))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 64);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Gemm_14_conv_18 */
  Conv_sw_info conv13_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 84,
    .general.input.dim.num_elem = 84,
    .general.input.stride.b = 336,
    .general.input.stride.h = 336,
    .general.input.stride.w = 336,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 336))) /* Equivalent hex address = 0x342e0150UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 10,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 84,
    .weights.dim.num_elem = 840,
    .weights.stride.b = 336,
    .weights.stride.h = 336,
    .weights.stride.w = 336,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 241920))) /* Equivalent hex address = 0x7103b100UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 10,
    .general.output.dim.num_elem = 10,
    .general.output.stride.b = 40,
    .general.output.stride.h = 40,
    .general.output.stride.w = 40,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Gemm_14_conv_18 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv13_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 64))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 64);

}


/* scheduling epoch=20   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=21   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_21(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start or end address (only line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1088))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1088))) /* Equivalent hex address = 0x342e0440UL */, 64);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Gemm_14_add_20 */
  Arith_sw_info arith14_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 10,
    .general.input.stride.b = 40,
    .general.input.stride.h = 40,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 10,
    .operand.stride.b = 40,
    .operand.stride.h = 40,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 246768))) /* Equivalent hex address = 0x7103c3f0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 10,
    .general.output.stride.b = 40,
    .general.output.stride.h = 40,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1088))) /* Equivalent hex address = 0x342e0440UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Gemm_14_add_20 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith14_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1088))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1088))) /* Equivalent hex address = 0x342e0440UL */, 64);

}


/* scheduling epoch=22   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_22(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1056))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1088))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1056))) /* Equivalent hex address = 0x342e0420UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Softmax node=Softmax_15 */
  Softmax_sw_info softmax15_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 10,
    .general.input.stride.b = 40,
    .general.input.stride.h = 40,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1088))) /* Equivalent hex address = 0x342e0440UL */,
    .general.input.format.is_signed = 0,
    /* "scratch" tensor-related info: */
    .scratch.dim.tensor_b = 1,
    .scratch.dim.tensor_h = 1,
    .scratch.dim.tensor_w = 257,
    .scratch.dim.tensor_c = 1,
    .scratch.dim.num_elem = 257,
    .scratch.stride.b = 1028,
    .scratch.stride.h = 1028,
    .scratch.stride.w = 4,
    .scratch.stride.c = 4,
    .scratch.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .scratch.format.is_signed = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 10,
    .general.output.stride.b = 40,
    .general.output.stride.h = 40,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1040))) /* Equivalent hex address = 0x342e0410UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .axis = 3,
    .general.type = LL_SW_SOFTMAX,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Softmax_15 mapped on EmbedNets (FLOAT) as Softmax | Category: Computational */
  ll_sw_forward_softmax(&softmax15_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1088))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 1088);

}


/* scheduling epoch=23   nodes=1   ------------------------------------------------------------------- */

/* scheduling DONE                 ------------------------------------------------------------------- */

const EpochBlock_ItemTypeDef *LL_ATON_EpochBlockItems_Default(void) {

  static const EpochBlock_ItemTypeDef ll_atonn_rt_epoch_block_array[] = {
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_2,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 2,
      .last_epoch_num = 2,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_3,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 3,
      .last_epoch_num = 3,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_4,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 4,
      .last_epoch_num = 4,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_5,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 5,
      .last_epoch_num = 5,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_6,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 6,
      .last_epoch_num = 6,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_7,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 7,
      .last_epoch_num = 7,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_9,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 9,
      .last_epoch_num = 9,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_11,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 11,
      .last_epoch_num = 11,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_12,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 12,
      .last_epoch_num = 12,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_14,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 14,
      .last_epoch_num = 14,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_16,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 16,
      .last_epoch_num = 16,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_17,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 17,
      .last_epoch_num = 17,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_19,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 19,
      .last_epoch_num = 19,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_21,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 21,
      .last_epoch_num = 21,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_22,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 22,
      .last_epoch_num = 22,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .flags = EpochBlock_Flags_last_eb,
    },
  };


  return ll_atonn_rt_epoch_block_array;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Input_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_28_28_1[] = { 1, 28, 1, 28 };
  static const uint32_t buff_info__mem_shape_F_1_28_28_1[] = { 1, 28, 28, 1 };
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const uint32_t buff_info__shape_6_1_5_5[] = { 6, 5, 5, 1 };
  static const uint32_t buff_info__mem_shape_F_6_1_5_5[] = { 6, 1, 5, 5 };
  static const uint32_t buff_info__shape_6[] = { 1, 1, 6, 1 };
  static const uint32_t buff_info__mem_shape_U_6[] = { 6 };
  static const uint32_t buff_info__shape_16_6_5_5[] = { 16, 5, 5, 6 };
  static const uint32_t buff_info__mem_shape_L_16_6_5_5[] = { 16, 5, 5, 6 };
  static const uint32_t buff_info__shape_16[] = { 1, 1, 16, 1 };
  static const uint32_t buff_info__mem_shape_U_16[] = { 16 };
  static const uint32_t buff_info__shape_120[] = { 1, 1, 120, 1 };
  static const uint32_t buff_info__mem_shape_U_120[] = { 120 };
  static const uint32_t buff_info__shape_84[] = { 1, 1, 84, 1 };
  static const uint32_t buff_info__mem_shape_U_84[] = { 84 };
  static const uint32_t buff_info__shape_10[] = { 1, 1, 10, 1 };
  static const uint32_t buff_info__mem_shape_U_10[] = { 10 };
  static const uint32_t buff_info__shape_120_400_1_1[] = { 120, 1, 1, 400 };
  static const uint32_t buff_info__mem_shape_F_120_400_1_1[] = { 120, 400, 1, 1 };
  static const uint32_t buff_info__shape_84_120_1_1[] = { 84, 1, 1, 120 };
  static const uint32_t buff_info__mem_shape_F_84_120_1_1[] = { 84, 120, 1, 1 };
  static const uint32_t buff_info__shape_10_84_1_1[] = { 10, 1, 1, 84 };
  static const uint32_t buff_info__mem_shape_F_10_84_1_1[] = { 10, 84, 1, 1 };
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_0_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 3136,
      .offset_limit = 3200,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_28_28_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_28_1,
    },
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = "Conv2D_2_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 245280,
      .offset_end = 245880,
      .offset_limit = 245944,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_6_1_5_5,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_6_1_5_5,
    },
    {
      .name = "Conv2D_2_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 246816,
      .offset_end = 246840,
      .offset_limit = 246904,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_6,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_6,
    },
    {
      .name = "Conv2D_5_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 232320,
      .offset_end = 241920,
      .offset_limit = 241984,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 6,
      .mem_shape = buff_info__mem_shape_L_16_6_5_5,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16_6_5_5,
    },
    {
      .name = "Conv2D_5_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 246704,
      .offset_end = 246768,
      .offset_limit = 246832,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_16,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16,
    },
    {
      .name = "Gemm_10_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 245888,
      .offset_end = 246368,
      .offset_limit = 246432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_120,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_120,
    },
    {
      .name = "Gemm_12_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 246368,
      .offset_end = 246704,
      .offset_limit = 246768,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_84,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_84,
    },
    {
      .name = "Gemm_14_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 246768,
      .offset_end = 246808,
      .offset_limit = 246872,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_10,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_10,
    },
    {
      .name = "Gemm_10_weights_transposed_3",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 0,
      .offset_end = 192000,
      .offset_limit = 192064,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 400,
      .mem_shape = buff_info__mem_shape_F_120_400_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_120_400_1_1,
    },
    {
      .name = "Gemm_12_weights_transposed_10",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 192000,
      .offset_end = 232320,
      .offset_limit = 232384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 120,
      .mem_shape = buff_info__mem_shape_F_84_120_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_84_120_1_1,
    },
    {
      .name = "Gemm_14_weights_transposed_17",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 241920,
      .offset_end = 245280,
      .offset_limit = 245344,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 84,
      .mem_shape = buff_info__mem_shape_F_10_84_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_10_84_1_1,
    },
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Output_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_10[] = { 1, 1, 10, 1 };
  static const uint32_t buff_info__mem_shape_U_1_10[] = { 1, 10 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Softmax_15_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 1040,
      .offset_end = 1080,
      .offset_limit = 1144,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 22,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_10,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_10,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Internal_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_1_28_28[] = { 1, 28, 28, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_28_28[] = { 1, 1, 28, 28 };
  static const uint32_t buff_info__shape_1_6_28_28[] = { 1, 28, 28, 6 };
  static const uint32_t buff_info__mem_shape_L_1_6_28_28[] = { 1, 28, 28, 6 };
  static const uint32_t buff_info__shape_1_6_14_14[] = { 1, 14, 14, 6 };
  static const uint32_t buff_info__mem_shape_L_1_6_14_14[] = { 1, 14, 14, 6 };
  static const uint32_t buff_info__shape_1_16_10_10[] = { 1, 10, 10, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_10_10[] = { 1, 10, 10, 16 };
  static const uint32_t buff_info__shape_1_16_5_5[] = { 1, 5, 5, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_5_5[] = { 1, 5, 5, 16 };
  static const uint32_t buff_info__shape_1_5_5_16[] = { 1, 5, 16, 5 };
  static const uint32_t buff_info__mem_shape_F_1_5_5_16[] = { 1, 5, 5, 16 };
  static const uint32_t buff_info__shape_1_400_1_1[] = { 1, 1, 1, 400 };
  static const uint32_t buff_info__mem_shape_F_1_400_1_1[] = { 1, 400, 1, 1 };
  static const uint32_t buff_info__shape_1_120_1_1[] = { 1, 1, 1, 120 };
  static const uint32_t buff_info__mem_shape_F_1_120_1_1[] = { 1, 120, 1, 1 };
  static const uint32_t buff_info__shape_1_120[] = { 1, 1, 120, 1 };
  static const uint32_t buff_info__mem_shape_U_1_120[] = { 1, 120 };
  static const uint32_t buff_info__shape_1_84_1_1[] = { 1, 1, 1, 84 };
  static const uint32_t buff_info__mem_shape_F_1_84_1_1[] = { 1, 84, 1, 1 };
  static const uint32_t buff_info__shape_1_84[] = { 1, 1, 84, 1 };
  static const uint32_t buff_info__mem_shape_U_1_84[] = { 1, 84 };
  static const uint32_t buff_info__shape_1_10_1_1[] = { 1, 1, 1, 10 };
  static const uint32_t buff_info__mem_shape_F_1_10_1_1[] = { 1, 10, 1, 1 };
  static const uint32_t buff_info__shape_1_10[] = { 1, 1, 10, 1 };
  static const uint32_t buff_info__mem_shape_U_1_10[] = { 1, 10 };
  static const uint32_t buff_info__shape_1_1_1_257[] = { 1, 1, 257, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_1_257[] = { 1, 1, 1, 257 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Transpose_1_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 3136,
      .offset_limit = 3200,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 1,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_28_28,
    },
    {
      .name = "Conv2D_2_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 3136,
      .offset_end = 21952,
      .offset_limit = 22016,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 2,
      .batch = 6,
      .mem_shape = buff_info__mem_shape_L_1_6_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_6_28_28,
    },
    {
      .name = "Tanh_3_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 3136,
      .offset_end = 21952,
      .offset_limit = 22016,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 3,
      .batch = 6,
      .mem_shape = buff_info__mem_shape_L_1_6_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_6_28_28,
    },
    {
      .name = "AveragePool_4_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 21952,
      .offset_end = 26656,
      .offset_limit = 26720,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 4,
      .batch = 6,
      .mem_shape = buff_info__mem_shape_L_1_6_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_6_14_14,
    },
    {
      .name = "Conv2D_5_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6400,
      .offset_limit = 6464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 5,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_10_10,
    },
    {
      .name = "Tanh_6_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6400,
      .offset_limit = 6464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 6,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_10_10,
    },
    {
      .name = "AveragePool_7_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6400,
      .offset_end = 8000,
      .offset_limit = 8064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 7,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_5_5,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_5_5,
    },
    {
      .name = "Transpose_8_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6400,
      .offset_end = 8000,
      .offset_limit = 8064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 8,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_5_5_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_5_5_16,
    },
    {
      .name = "Gemm_10_reshape_x_2",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6400,
      .offset_end = 8000,
      .offset_limit = 8064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 8,
      .batch = 400,
      .mem_shape = buff_info__mem_shape_F_1_400_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_400_1_1,
    },
    {
      .name = "Gemm_10_conv_4",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 480,
      .offset_limit = 544,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 9,
      .batch = 120,
      .mem_shape = buff_info__mem_shape_F_1_120_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_120_1_1,
    },
    {
      .name = "Gemm_10_squeeze_y_5",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 480,
      .offset_limit = 544,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 10,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_120,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_120,
    },
    {
      .name = "Gemm_10_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 480,
      .offset_end = 960,
      .offset_limit = 1024,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 11,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_120,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_120,
    },
    {
      .name = "Tanh_11_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 480,
      .offset_end = 960,
      .offset_limit = 1024,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 12,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_120,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_120,
    },
    {
      .name = "Gemm_12_reshape_x_9",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 480,
      .offset_end = 960,
      .offset_limit = 1024,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 13,
      .batch = 120,
      .mem_shape = buff_info__mem_shape_F_1_120_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_120_1_1,
    },
    {
      .name = "Gemm_12_conv_11",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 336,
      .offset_limit = 400,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 14,
      .batch = 84,
      .mem_shape = buff_info__mem_shape_F_1_84_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_84_1_1,
    },
    {
      .name = "Gemm_12_squeeze_y_12",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 336,
      .offset_limit = 400,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 15,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_84,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_84,
    },
    {
      .name = "Gemm_12_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 336,
      .offset_end = 672,
      .offset_limit = 736,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 16,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_84,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_84,
    },
    {
      .name = "Tanh_13_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 336,
      .offset_end = 672,
      .offset_limit = 736,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 17,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_84,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_84,
    },
    {
      .name = "Gemm_14_reshape_x_16",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 336,
      .offset_end = 672,
      .offset_limit = 736,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 18,
      .batch = 84,
      .mem_shape = buff_info__mem_shape_F_1_84_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_84_1_1,
    },
    {
      .name = "Gemm_14_conv_18",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 40,
      .offset_limit = 104,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 19,
      .batch = 10,
      .mem_shape = buff_info__mem_shape_F_1_10_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_10_1_1,
    },
    {
      .name = "Gemm_14_squeeze_y_19",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 40,
      .offset_limit = 104,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 20,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_10,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_10,
    },
    {
      .name = "Gemm_14_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 1088,
      .offset_end = 1128,
      .offset_limit = 1192,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 21,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_10,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_10,
    },
    {
      .name = "SCRATCH_Softmax_15_PORT_OUT",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 1028,
      .offset_limit = 1096,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 22,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_1_257,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_1_257,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

